{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQoPRuWK0Ysi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHNfJxhEwgo3"
      },
      "outputs": [],
      "source": [
        "import tokenizers\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.pre_tokenizers import WhitespaceSplit,Split\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.pre_tokenizers import Punctuation, WhitespaceSplit\n",
        "from tokenizers.normalizers import Lowercase, Replace\n",
        "from tokenizers import Regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDznGW7rYT9a"
      },
      "outputs": [],
      "source": [
        "def train_tokenizer() -> tokenizers.Tokenizer:\n",
        "    PAD_TOKEN = '<pad>'\n",
        "    UNK_TOKEN = '<unk>'\n",
        "    NUM_TOKEN = '<num>'\n",
        "    START_TOKEN= '<bos>'\n",
        "    END_TOKEN= '<eos>'\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
        "    tokenizer.normalizer = normalizers.Sequence([normalizers.Lowercase(),\n",
        "                                                 normalizers.Replace(Regex(r'[^\\w\\s]'), \"\"),\n",
        "                                                 normalizers.Replace(Regex('[0-9]+'), NUM_TOKEN)\n",
        "                                                 ])\n",
        "    trainer = WordLevelTrainer(special_tokens=[PAD_TOKEN,UNK_TOKEN,NUM_TOKEN,START_TOKEN,END_TOKEN])\n",
        "    tokenizer.pre_tokenizer = WhitespaceSplit()\n",
        "    files = [\"amazon_train.txt\",\"deloitte_train.txt\",\"google_train.txt\",\"meta_train.txt\"]\n",
        "    tokenizer.train(files, trainer)\n",
        "\n",
        "    tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<bos> $A <eos>\",\n",
        "    special_tokens=[\n",
        "        (START_TOKEN, tokenizer.token_to_id(START_TOKEN)),\n",
        "        (END_TOKEN, tokenizer.token_to_id(END_TOKEN)),\n",
        "         ],\n",
        "    )\n",
        "    # tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4auJ-V8WZHHP",
        "outputId": "5b0645ca-fbdd-4478-94f6-a70571a7df51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11845\n"
          ]
        }
      ],
      "source": [
        "tokenizer = train_tokenizer()\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMMr1eLO6mhm"
      },
      "outputs": [],
      "source": [
        "def token_gen(fname: str,\n",
        "                  tokenizer: tokenizers.Tokenizer) :\n",
        "    return_sents = []\n",
        "    all_tokens = []\n",
        "    with open(fname, \"r\", encoding=\"utf-8\") as file:\n",
        "        sentences = file.readlines()\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer.encode(sentence).tokens\n",
        "        line = \" \".join(tokens)\n",
        "        return_sents.append(line+\"\\n\")\n",
        "    return return_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISHVeRyF7aFX"
      },
      "outputs": [],
      "source": [
        "sents_amazon = token_gen(\"amazon.txt\",tokenizer)\n",
        "\n",
        "sents_deloitte = token_gen(\"deloitte.txt\",tokenizer)\n",
        "sents_meta = token_gen(\"meta.txt\",tokenizer)\n",
        "sents_google = token_gen(\"google.txt\",tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sYV4_XuzyK_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "def sentiment_analysis(fname,model) :\n",
        "    #specific_model = pipeline(model=\"Kaludi/Reviews-Sentiment-Analysis\")\n",
        "    resultant =[]\n",
        "    for sent in fname:\n",
        "      output = model(sent)\n",
        "      x= output[0]['label']\n",
        "      y = output[0]['score']\n",
        "      resultant.append([sent,x,y])\n",
        "    return resultant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEU15sd3s7SH"
      },
      "outputs": [],
      "source": [
        "model = pipeline(model=\"Kaludi/Reviews-Sentiment-Analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8b1er7-s7SH"
      },
      "outputs": [],
      "source": [
        "resultant_amazon = sentiment_analysis(sents_amazon,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geNrYRKNs7SI"
      },
      "outputs": [],
      "source": [
        "resultant_deloitte = sentiment_analysis(sents_deloitte,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT3-lMlhs7SI"
      },
      "outputs": [],
      "source": [
        "resultant_meta = sentiment_analysis(sents_meta,model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lUFV2P1s7SI"
      },
      "outputs": [],
      "source": [
        "resultant_google = sentiment_analysis(sents_google,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFBvV3Bis7SI",
        "outputId": "2b334b4a-35b6-4f22-fc4e-494c35095925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<bos> so this is my first week at amazon i am completing my <num>th day of work and i absolutely love it looking forward to my day off lol <eos>\\n', 'Positive', 0.9855977892875671]\n"
          ]
        }
      ],
      "source": [
        "print(resultant_amazon[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNjPE_OTs7SI",
        "outputId": "72b2aac5-a760-4cc5-e855-9660cbddfa36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Confidence: 0.8187582866704701\n",
            "0.919755877034358\n"
          ]
        }
      ],
      "source": [
        "columns = ['Text', 'Sentiment', 'Confidence']\n",
        "df_amazon = pd.DataFrame(resultant_amazon, columns=columns)\n",
        "average_confidence = df_amazon['Confidence'].mean()\n",
        "print(\"Average Confidence:\", average_confidence)\n",
        "\n",
        "positive_count = df_amazon['Sentiment'].value_counts().get('Positive', 0)\n",
        "Positive_percentage = positive_count/len(df_amazon)\n",
        "print(Positive_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsyYn0pEs7SI",
        "outputId": "1a03f440-0914-418d-ef25-1688dba8d4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Confidence: 0.8340425815196396\n",
            "0.9482177263969171\n"
          ]
        }
      ],
      "source": [
        "columns = ['Text', 'Sentiment', 'Confidence']\n",
        "df_deloitte = pd.DataFrame(resultant_deloitte, columns=columns)\n",
        "average_confidence = df_deloitte['Confidence'].mean()\n",
        "print(\"Average Confidence:\", average_confidence)\n",
        "\n",
        "positive_count = df_deloitte['Sentiment'].value_counts().get('Positive', 0)\n",
        "Positive_percentage = positive_count/len(df_deloitte)\n",
        "print(Positive_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sxBKoaOs7SI",
        "outputId": "2dccae86-8624-457a-925a-5d2fa9072cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Confidence: 0.8124302262804981\n",
            "0.9216904909881914\n"
          ]
        }
      ],
      "source": [
        "columns = ['Text', 'Sentiment', 'Confidence']\n",
        "df_meta = pd.DataFrame(resultant_meta, columns=columns)\n",
        "average_confidence = df_meta['Confidence'].mean()\n",
        "print(\"Average Confidence:\", average_confidence)\n",
        "\n",
        "positive_count = df_meta['Sentiment'].value_counts().get('Positive', 0)\n",
        "Positive_percentage = positive_count/len(df_meta)\n",
        "print(Positive_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SBvH7oas7SI",
        "outputId": "1be53bb5-eff9-462b-c12c-c92cd92f5006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Confidence: 0.8077500297998389\n",
            "0.9010416666666666\n"
          ]
        }
      ],
      "source": [
        "columns = ['Text', 'Sentiment', 'Confidence']\n",
        "df_google = pd.DataFrame(resultant_google, columns=columns)\n",
        "average_confidence = df_google['Confidence'].mean()\n",
        "print(\"Average Confidence:\", average_confidence)\n",
        "\n",
        "positive_count = df_google['Sentiment'].value_counts().get('Positive', 0)\n",
        "Positive_percentage = positive_count/len(df_google)\n",
        "print(Positive_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW8s8OKYs7SI"
      },
      "outputs": [],
      "source": [
        "filtered_df_amazon = df_amazon[df_amazon['Sentiment'] == 'Negative']\n",
        "filtered_df_deloitte = df_deloitte[df_deloitte['Sentiment'] == 'Negative']\n",
        "filtered_df_meta = df_meta[df_meta['Sentiment'] == 'Negative']\n",
        "filtered_df_google = df_google[df_google['Sentiment'] == 'Negative']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hE4FjLPs7SJ"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "def token_filter(sentences, thresh=5):\n",
        "    vocab = []\n",
        "    tokens=[]\n",
        "    sents = sentences\n",
        "\n",
        "    for i in range(len(sents)):\n",
        "          tokens += sents[i].split(\" \")\n",
        "\n",
        "    fq = FreqDist(token.lower() for token in tokens)\n",
        "\n",
        "    filtered_tokens = [token for token in tokens if (fq[token] <= 15) ]\n",
        "    vocab = list(set(filtered_tokens))\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdSprS_us7SJ"
      },
      "outputs": [],
      "source": [
        "my_list_amazon = list(filtered_df_amazon['Text'])\n",
        "vocab_amazon = token_filter(my_list_amazon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdSa0NYWs7SJ"
      },
      "outputs": [],
      "source": [
        "my_list_deloitte = list(filtered_df_deloitte['Text'])\n",
        "vocab_deloitte = token_filter(my_list_deloitte)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LnYfWJds7SJ"
      },
      "outputs": [],
      "source": [
        "my_list_meta = list(filtered_df_meta['Text'])\n",
        "vocab_meta = token_filter(my_list_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzg4lmIns7SJ"
      },
      "outputs": [],
      "source": [
        "my_list_google = list(filtered_df_google['Text'])\n",
        "vocab_google = token_filter(my_list_google)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5VUB09Qs7SJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import  numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "stopwords = set(STOPWORDS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4KG5r71s7SJ"
      },
      "outputs": [],
      "source": [
        "filtered_df_amazon = pd.DataFrame({'Text':filtered_df_amazon.Text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBbzu4KEs7SJ"
      },
      "outputs": [],
      "source": [
        "filtered_df_deloitte = pd.DataFrame({'Text':filtered_df_deloitte.Text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cC8awW4s7SJ"
      },
      "outputs": [],
      "source": [
        "filtered_df_meta = pd.DataFrame({'Text':filtered_df_meta.Text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yuf-hiqfs7SJ"
      },
      "outputs": [],
      "source": [
        "filtered_df_google = pd.DataFrame({'Text':filtered_df_google.Text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEut5ynTs7SJ"
      },
      "outputs": [],
      "source": [
        "comment_words = ''\n",
        "f = open(\"comment_words_amazon_neg.txt\",'w')\n",
        "for val in filtered_df_amazon['Text']:\n",
        "    tokens = val.lower().split()\n",
        "    tokens=[value for value in tokens if value != '<bos>' and value != '<eos>' and value != '<num>' and value != '<unk>' and value not in vocab_amazon ]\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "    f.write(\" \".join(tokens)+\" \")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcmJX3Cos7SJ"
      },
      "outputs": [],
      "source": [
        "comment_words = ''\n",
        "f = open(\"comment_words_deloitte_neg.txt\",'w')\n",
        "for val in filtered_df_deloitte['Text']:\n",
        "    tokens = val.lower().split()\n",
        "    tokens=[value for value in tokens if value != '<bos>' and value != '<eos>' and value != '<num>' and value != '<unk>' and value not in vocab_deloitte ]\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "    f.write(\" \".join(tokens)+\" \")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0o4oHhZs7SJ"
      },
      "outputs": [],
      "source": [
        "comment_words = ''\n",
        "f = open(\"comment_words_meta_neg.txt\",'w')\n",
        "for val in filtered_df_meta['Text']:\n",
        "    tokens = val.lower().split()\n",
        "    tokens=[value for value in tokens if value != '<bos>' and value != '<eos>' and value != '<num>' and value != '<unk>' and value not in vocab_meta ]\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "    f.write(\" \".join(tokens)+\" \")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pOt10_us7SK"
      },
      "outputs": [],
      "source": [
        "comment_words = ''\n",
        "f = open(\"comment_words_google_neg.txt\",'w')\n",
        "for val in filtered_df_google['Text']:\n",
        "    tokens = val.lower().split()\n",
        "    tokens=[value for value in tokens if value != '<bos>' and value != '<eos>' and value != '<num>' and value != '<unk>' and value not in vocab_google ]\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "    f.write(\" \".join(tokens)+\" \")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCv4eC8ss7SK"
      },
      "source": [
        "## POSITIVE COMMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiHh__wUs7SK"
      },
      "outputs": [],
      "source": [
        "filtered_df_amazon = df_amazon[df_amazon['Sentiment'] != 'Negative']\n",
        "filtered_df_deloitte = df_deloitte[df_deloitte['Sentiment'] != 'Negative']\n",
        "filtered_df_meta = df_meta[df_meta['Sentiment'] != 'Negative']\n",
        "filtered_df_google = df_google[df_google['Sentiment'] != 'Negative']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI5sc1mgs7SK"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "def token_filter(sentences, thresh=5):\n",
        "    vocab = []\n",
        "    tokens=[]\n",
        "    sents = sentences\n",
        "\n",
        "    for i in range(len(sents)):\n",
        "          tokens += sents[i].split(\" \")\n",
        "\n",
        "    fq = FreqDist(token.lower() for token in tokens)\n",
        "\n",
        "    filtered_tokens = [token for token in tokens if (fq[token] <= 10) ]\n",
        "    vocab = list(set(filtered_tokens))\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvBhkCDDs7SK"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import  numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "def gen_word_cloud(df,fname):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    my_list = list(df['Text'])\n",
        "    vocab = token_filter(my_list)\n",
        "    filtered_df = pd.DataFrame({'Text':df.Text})\n",
        "    comment_words = ''\n",
        "    f = open(fname,'w')\n",
        "    for val in df['Text']:\n",
        "        tokens = val.lower().split()\n",
        "        tokens=[value for value in tokens if value != '<bos>' and value != '<eos>' and value != '<num>' and value != '<unk>' and value not in vocab and value not in stopwords]\n",
        "        comment_words += \" \".join(tokens)+\" \"\n",
        "        f.write(\" \".join(tokens)+\" \")\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Ktf_Zvs7SR"
      },
      "outputs": [],
      "source": [
        "gen_word_cloud(filtered_df_amazon,\"comment_words_amazon_pos.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ulHIn3s7SR"
      },
      "outputs": [],
      "source": [
        "gen_word_cloud(filtered_df_deloitte,\"comment_words_deloitte_pos.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ_HkJyus7SR"
      },
      "outputs": [],
      "source": [
        "gen_word_cloud(filtered_df_meta,\"comment_words_meta_pos.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXOpajy2s7SR"
      },
      "outputs": [],
      "source": [
        "gen_word_cloud(filtered_df_google,\"comment_words_google_pos.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYCAR1Yys7SR"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33ctd8fTs7SR"
      },
      "outputs": [],
      "source": [
        "entire_dataset = sents_amazon +sents_deloitte+sents_meta+sents_google\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "def token_filter(dataset, thresh=5):\n",
        "    vocab = []\n",
        "    tokens=[]\n",
        "    #Read the tokens from the file\n",
        "    sents = dataset\n",
        "    filtered_sents=[]\n",
        "    for i in range(len(sents)):\n",
        "      tokens += sents[i].split(\" \")\n",
        "    # calculate the frequency of the words\n",
        "    fq = FreqDist(token.lower() for token in tokens)\n",
        "    filtered_tokens = [token for token in tokens if fq[token] >= 5 and token != '<bos>' and token != '<eos>' and token != '<num>' and token != '<unk>' ]\n",
        "    vocab = list(set(filtered_tokens))\n",
        "    for i in range(len(sents)-2) :\n",
        "      words = sents[i].split(\" \")\n",
        "      filtered_words = [token for token in words if fq[token] >= 5 and token != '<bos>' and token != '<eos>' and token != '<num>' and token != '<unk>' ]\n",
        "      text = \" \".join(filtered_words)\n",
        "      filtered_sents.append(text)\n",
        "    return vocab,filtered_sents\n",
        "\n",
        "vocab,dataset = token_filter(entire_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUn9hPMxs7SR"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def LSA(dataset, dim=50):\n",
        "    sents = dataset\n",
        "    vectorizer = CountVectorizer(tokenizer=lambda x: x.split(), lowercase=False, binary=False)\n",
        "    word_doc_matrix = vectorizer.fit_transform(sents)\n",
        "    word_doc_matrix=word_doc_matrix.transpose()\n",
        "    svd = TruncatedSVD(n_components=50)\n",
        "    embeddings = svd.fit_transform(word_doc_matrix)\n",
        "    vocab = {word:index for index, word in enumerate(vectorizer.get_feature_names_out())}\n",
        "    return embeddings,vocab\n",
        "embeddings_dataset,vocab_dataset=LSA(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmZz1BGws7SR"
      },
      "outputs": [],
      "source": [
        "vocab_amazon,dataset_amazon = token_filter(sents_amazon)\n",
        "embeddings_amazon,vocab_amazon = LSA(dataset_amazon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yZ--2bys7SR"
      },
      "outputs": [],
      "source": [
        "vocab_deloitte,dataset_deloitte = token_filter(sents_deloitte)\n",
        "embeddings_deloitte,vocab_deloitte = LSA(dataset_deloitte)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U36_hMQs7SR"
      },
      "outputs": [],
      "source": [
        "vocab_meta,dataset_meta = token_filter(sents_meta)\n",
        "embeddings_meta,vocab_meta = LSA(dataset_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MrHU3Mzs7SR"
      },
      "outputs": [],
      "source": [
        "vocab_google,dataset_google = token_filter(sents_google)\n",
        "embeddings_google,vocab_google = LSA(dataset_google)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7yQg3yhs7SR"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def correlation(word, embeddings_dataset,embeddings_company,vocab_dataset,vocab_company):\n",
        "    if word in vocab_dataset and word in vocab_company:\n",
        "        index_dataset = vocab_dataset[word]\n",
        "        index_company = vocab_company[word]\n",
        "    embedding_dataset = embeddings_dataset[index_dataset]\n",
        "    embedding_company = embeddings_company[index_company]\n",
        "\n",
        "    embedding_dataset = embedding_dataset.reshape(1, -1)\n",
        "    embedding_company = embedding_company.reshape(1, -1)\n",
        "    cosine_similarity_score = cosine_similarity(embedding_dataset, embedding_company)[0][0]\n",
        "    return cosine_similarity_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa6i8bzLs7SS"
      },
      "outputs": [],
      "source": [
        "x = correlation(\"work\",embeddings_dataset,embeddings_amazon,vocab_dataset,vocab_amazon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK1OyA3is7SS",
        "outputId": "c967d36c-a6cd-4d3d-b5cb-7b5f56b493f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.48048370628613357\n"
          ]
        }
      ],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT3RCbqLs7SS",
        "outputId": "0e9808ca-5bac-4923-c4bd-c253e6da2da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive word correlation\n",
            "Amazon: word = work and correlation = 0.48048370628613357\n",
            "Deloitte: word = work and correlation = 0.14394640271696085\n",
            "Meta: word = work and correlation = 0.1733925800374058\n",
            "Google: word = work and correlation = 0.36960640316949367\n",
            "\n",
            "Amazon: word = job and correlation = -0.13325785120658215\n",
            "Deloitte: word = job and correlation = 0.1677679684670904\n",
            "Meta: word = job and correlation = 0.0515980033161018\n",
            "Google: word = job and correlation = 0.2737629760311749\n",
            "\n",
            "Amazon: word = pay and correlation = 0.2689717678033564\n",
            "Deloitte: word = pay and correlation = 0.3601465736769758\n",
            "Meta: word = pay and correlation = 0.38293269688758313\n",
            "Google: word = pay and correlation = 0.45323462663855557\n",
            "\n",
            "Amazon: word = position and correlation = 0.36924063566828874\n",
            "Deloitte: word = position and correlation = 0.2672624552595968\n",
            "Meta: word = position and correlation = 0.264023754607288\n",
            "Google: word = position and correlation = 0.32063112888045914\n",
            "\n",
            "Amazon: word = thank and correlation = -0.06837659767774112\n",
            "Deloitte: word = thank and correlation = 0.07511460485237564\n",
            "Meta: word = thank and correlation = 0.03285223682226607\n",
            "Google: word = thank and correlation = 0.06633448850594555\n",
            "\n",
            "Negative word correlation\n",
            "Amazon: word = people and correlation = 0.30426190583223184\n",
            "Deloitte: word = people and correlation = 0.25112726266087165\n",
            "Meta: word = people and correlation = 0.460385503250899\n",
            "Google: word = people and correlation = 0.3167393385411826\n",
            "\n",
            "Amazon: word = experience and correlation = 0.35809590542921244\n",
            "Deloitte: word = experience and correlation = 0.47727655102126976\n",
            "Meta: word = experience and correlation = 0.23752761349083798\n",
            "Google: word = experience and correlation = 0.22771760878534933\n",
            "\n",
            "Amazon: word = manager and correlation = 0.47093956202237836\n",
            "Deloitte: word = manager and correlation = 0.36606752138455256\n",
            "Meta: word = manager and correlation = 0.38548118329320397\n",
            "Google: word = manager and correlation = 0.24979502449349802\n",
            "\n",
            "Amazon: word = hr and correlation = 0.27282859039331375\n",
            "Deloitte: word = hr and correlation = 0.13929035157863534\n",
            "Meta: word = hr and correlation = 0.25743853334692685\n",
            "Google: word = hr and correlation = 0.14926223269472738\n",
            "\n",
            "Amazon: word = company and correlation = 0.3688933692934273\n",
            "Deloitte: word = company and correlation = 0.4881987198372977\n",
            "Meta: word = company and correlation = 0.4554861393458819\n",
            "Google: word = company and correlation = 0.4354966994788305\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Positive word correlation\")\n",
        "positive_words=['work','job','pay','position','thank']\n",
        "for word in positive_words:\n",
        "    print(\"Amazon: word = {} and correlation = {}\".format(word,correlation(word,embeddings_dataset,embeddings_amazon,vocab_dataset,vocab_amazon)))\n",
        "    print(\"Deloitte: word = {} and correlation = {}\".format(word,correlation(word,embeddings_dataset,embeddings_deloitte,vocab_dataset,vocab_deloitte)))\n",
        "    print(\"Meta: word = {} and correlation = {}\".format(word,correlation(word,embeddings_dataset,embeddings_meta,vocab_dataset,vocab_meta)))\n",
        "    print(\"Google: word = {} and correlation = {}\".format(word,correlation(word,embeddings_dataset,embeddings_google,vocab_dataset,vocab_google)))\n",
        "    print()\n",
        "\n",
        "print(\"Negative word correlation\")\n",
        "negative_words=['people','experience','manager','hr','company']\n",
        "for word in negative_words:\n",
        "    print(\"Amazon: word = {} and correlation = {}\".format(word,correlation(word,embeddings_dataset,embeddings_amazon,vocab_dataset,vocab_amazon)))\n",
        "    print(\"Deloitte: word = {} and correlation = {}\".format(word,correlation(word,embeddings_dataset,embeddings_deloitte,vocab_dataset,vocab_deloitte)))\n",
        "    print(\"Meta: word = {} and correlation = {}\".format(word,correlation(word,embeddings_dataset,embeddings_meta,vocab_dataset,vocab_meta)))\n",
        "    print(\"Google: word = {} and correlation = {}\".format(word,correlation(word,embeddings_dataset,embeddings_google,vocab_dataset,vocab_google)))\n",
        "    print()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riKFBgLZs7SS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8",
      "language": "python",
      "name": "python-3.8"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}